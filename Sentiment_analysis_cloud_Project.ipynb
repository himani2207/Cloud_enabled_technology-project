{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p36",
      "language": "python",
      "name": "conda_pytorch_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Sentiment_analysis_cloud_Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himani2207/Cloud_enabled_technology-project/blob/main/Sentiment_analysis_cloud_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymL32SvdTA06",
        "outputId": "2e5133f6-6fdf-47af-c236-e991cbc0b926"
      },
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-24 11:55:52--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  24.3MB/s    in 3.9s    \n",
            "\n",
            "2018-12-24 11:55:56 (20.3 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKvLXmncTA08"
      },
      "source": [
        "#preparing and preprocessing the data\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "    \n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "        \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "                    \n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "                \n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewVKyXXkTA09",
        "outputId": "a217d23d-974f-4dd0-c4f8-4e629419fcd0"
      },
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9bc3JOfTA0-"
      },
      "source": [
        "# combine the positive and negative reviews and shuffle the resulting records.\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wQU617CTA0_",
        "outputId": "28465b3c-8b80-4643-dc3b-ab22131c0c6d"
      },
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj26hYQYTA1B",
        "outputId": "1d6ab554-c572-4d04-8343-c3d90e1eb5ff"
      },
      "source": [
        "print(train_X[100])\n",
        "print(train_y[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before I'd seen this, I had seen some pretty bad Christmas films. But once I saw this, \"Jingle All the Way\" looked better than \"The Godfather\". \"Santa Claus\" is a jolly film about Santa helping out some kids, but it almost feels demonic watching it. Santa's jolly ho-ho-ho is replaces by an evil, devilish laugh that I'm sure has turned many kids off of Christmas. The plot of this massacre is very strange, which fits along with all of the performances and dialog. Santa lives high above Earth in the North Pole where he, and kids from all around the world get ready for Christmas. But Santa has an enemy named Pitch, or Satan. Pitch tries to ruin Santa's Christmas by making three boys naughty, and by creating diversions, like moving the chimney and making the doorknob hot. When Pitch causes Santa to be attacked by a dog, it's up to Santa's helper Pedro and Merlin the wizard to get Santa out of this pickle. <br /><br />Everything about this film, along with being downright bad, is so bizarre. Satan dances a lot and he actually seems much more merry than Santa. Santa talks about delivering presents to all the boys and girls, yet he seems to only deliver to 5 houses of kids in Mexico. The reindeer are wind up toys, and when the reindeer laughs, I'm amazed it doesn't bring tears to kid's eyes...it's frightening. Everything is terrible. The first 10 minutes are simply Santa playing the organ while kids sing to it. Probably one of the strangest scenes is Santa shooting Pitch in the butt with a mini-cannon and uproariously laughing about it while Pitch dances around in pain. I think parents are better off telling their little kids about where babies come from, than showing them this. The only positive is it will have you laughing hysterically if you can appreciate bad cinema.<br /><br />My rating: BOMB/****. 85 mins.\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbFgH18gTA1C"
      },
      "source": [
        "#any html tags that appear should be removed\n",
        "#tokenize the input\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#review_to_words method defined above uses BeautifulSoup to remove any html tags that appear\n",
        "def review_to_words(review):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sywr-aHTTA1D",
        "outputId": "800edd1a-7332-44c1-c635-90753f854808"
      },
      "source": [
        "# TODO: Apply review_to_words to a review (train_X[100] or any other review)\n",
        "review_to_words(train_X[100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['seen',\n",
              " 'seen',\n",
              " 'pretti',\n",
              " 'bad',\n",
              " 'christma',\n",
              " 'film',\n",
              " 'saw',\n",
              " 'jingl',\n",
              " 'way',\n",
              " 'look',\n",
              " 'better',\n",
              " 'godfath',\n",
              " 'santa',\n",
              " 'clau',\n",
              " 'jolli',\n",
              " 'film',\n",
              " 'santa',\n",
              " 'help',\n",
              " 'kid',\n",
              " 'almost',\n",
              " 'feel',\n",
              " 'demon',\n",
              " 'watch',\n",
              " 'santa',\n",
              " 'jolli',\n",
              " 'ho',\n",
              " 'ho',\n",
              " 'ho',\n",
              " 'replac',\n",
              " 'evil',\n",
              " 'devilish',\n",
              " 'laugh',\n",
              " 'sure',\n",
              " 'turn',\n",
              " 'mani',\n",
              " 'kid',\n",
              " 'christma',\n",
              " 'plot',\n",
              " 'massacr',\n",
              " 'strang',\n",
              " 'fit',\n",
              " 'along',\n",
              " 'perform',\n",
              " 'dialog',\n",
              " 'santa',\n",
              " 'live',\n",
              " 'high',\n",
              " 'earth',\n",
              " 'north',\n",
              " 'pole',\n",
              " 'kid',\n",
              " 'around',\n",
              " 'world',\n",
              " 'get',\n",
              " 'readi',\n",
              " 'christma',\n",
              " 'santa',\n",
              " 'enemi',\n",
              " 'name',\n",
              " 'pitch',\n",
              " 'satan',\n",
              " 'pitch',\n",
              " 'tri',\n",
              " 'ruin',\n",
              " 'santa',\n",
              " 'christma',\n",
              " 'make',\n",
              " 'three',\n",
              " 'boy',\n",
              " 'naughti',\n",
              " 'creat',\n",
              " 'divers',\n",
              " 'like',\n",
              " 'move',\n",
              " 'chimney',\n",
              " 'make',\n",
              " 'doorknob',\n",
              " 'hot',\n",
              " 'pitch',\n",
              " 'caus',\n",
              " 'santa',\n",
              " 'attack',\n",
              " 'dog',\n",
              " 'santa',\n",
              " 'helper',\n",
              " 'pedro',\n",
              " 'merlin',\n",
              " 'wizard',\n",
              " 'get',\n",
              " 'santa',\n",
              " 'pickl',\n",
              " 'everyth',\n",
              " 'film',\n",
              " 'along',\n",
              " 'downright',\n",
              " 'bad',\n",
              " 'bizarr',\n",
              " 'satan',\n",
              " 'danc',\n",
              " 'lot',\n",
              " 'actual',\n",
              " 'seem',\n",
              " 'much',\n",
              " 'merri',\n",
              " 'santa',\n",
              " 'santa',\n",
              " 'talk',\n",
              " 'deliv',\n",
              " 'present',\n",
              " 'boy',\n",
              " 'girl',\n",
              " 'yet',\n",
              " 'seem',\n",
              " 'deliv',\n",
              " '5',\n",
              " 'hous',\n",
              " 'kid',\n",
              " 'mexico',\n",
              " 'reindeer',\n",
              " 'wind',\n",
              " 'toy',\n",
              " 'reindeer',\n",
              " 'laugh',\n",
              " 'amaz',\n",
              " 'bring',\n",
              " 'tear',\n",
              " 'kid',\n",
              " 'eye',\n",
              " 'frighten',\n",
              " 'everyth',\n",
              " 'terribl',\n",
              " 'first',\n",
              " '10',\n",
              " 'minut',\n",
              " 'simpli',\n",
              " 'santa',\n",
              " 'play',\n",
              " 'organ',\n",
              " 'kid',\n",
              " 'sing',\n",
              " 'probabl',\n",
              " 'one',\n",
              " 'strangest',\n",
              " 'scene',\n",
              " 'santa',\n",
              " 'shoot',\n",
              " 'pitch',\n",
              " 'butt',\n",
              " 'mini',\n",
              " 'cannon',\n",
              " 'uproari',\n",
              " 'laugh',\n",
              " 'pitch',\n",
              " 'danc',\n",
              " 'around',\n",
              " 'pain',\n",
              " 'think',\n",
              " 'parent',\n",
              " 'better',\n",
              " 'tell',\n",
              " 'littl',\n",
              " 'kid',\n",
              " 'babi',\n",
              " 'come',\n",
              " 'show',\n",
              " 'posit',\n",
              " 'laugh',\n",
              " 'hyster',\n",
              " 'appreci',\n",
              " 'bad',\n",
              " 'cinema',\n",
              " 'rate',\n",
              " 'bomb',\n",
              " '85',\n",
              " 'min']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-9O-IElTA1G"
      },
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5o2i4_ETA1H",
        "outputId": "093e6486-1078-40d0-e463-d8de1b6e0ddd"
      },
      "source": [
        "# Preprocess data\n",
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yWUdK3UTA1N"
      },
      "source": [
        "import numpy as np\n",
        "def convert_and_pad(word_dict, sentence, pad=500):\n",
        "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
        "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
        "    \n",
        "    working_sentence = [NOWORD] * pad\n",
        "    \n",
        "    for word_index, word in enumerate(sentence[:pad]):\n",
        "        if word in word_dict:\n",
        "            working_sentence[word_index] = word_dict[word]\n",
        "        else:\n",
        "            working_sentence[word_index] = INFREQ\n",
        "            \n",
        "    return working_sentence, min(len(sentence), pad)\n",
        "\n",
        "def convert_and_pad_data(word_dict, data, pad=500):\n",
        "    result = []\n",
        "    lengths = []\n",
        "    \n",
        "    for sentence in data:\n",
        "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
        "        result.append(converted)\n",
        "        lengths.append(leng)\n",
        "        \n",
        "    return np.array(result), np.array(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEg5-tmgTA1N"
      },
      "source": [
        "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
        "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6II8F_ITTA1O",
        "outputId": "b79545c1-fc25-4a2b-bdbf-c11d765a6812"
      },
      "source": [
        "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
        "train_X[20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 135,    2,  424,  838,  168, 1345,   37,  298,    4,  729,    1,\n",
              "          1, 3508, 2982,  346, 2032,  178,   94,  181,   15,  165,  423,\n",
              "          1,   52,    3,  433,  156,  353,  383, 4910, 1380, 3508, 2982,\n",
              "         76,   73,  161,  770,  321,  403,    1,    1,  895, 3417,  235,\n",
              "       2025, 1826,  586,    1, 3508,   33, 4095, 2014,    1, 1108,  241,\n",
              "        723,    8,  725,  329, 3508,  122,   16,    1, 1826,    1,  312,\n",
              "          4,  283, 1871,   52,  247,  237,  725,  400, 3376, 2062,  486,\n",
              "          3,    1,   65,    1,  338,   33,  642,   81,    1,   99,  465,\n",
              "         31,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rziXakhPTA1O"
      },
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JZNvVKyTA1P"
      },
      "source": [
        "import pandas as pd\n",
        "    \n",
        "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
        "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zf26DcUTA1Q",
        "outputId": "667c1637-1c34-4759-8f4e-d1777db7e2cd"
      },
      "source": [
        "import sagemaker\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "prefix = 'sagemaker/sentiment_rnn'\n",
        "\n",
        "role = sagemaker.get_execution_role()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-805470203735\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkcgvSxfTA1Q"
      },
      "source": [
        "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z51l7s5kTA1R",
        "outputId": "4d6b9f34-68b9-4ccf-d6b9-ff01f48f280b"
      },
      "source": [
        "!pygmentize train/model.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
            "\r\n",
            "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
            "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
            "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
            "\r\n",
            "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
            "        \r\n",
            "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[36mNone\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        x = x.t()\r\n",
            "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
            "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
            "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
            "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
            "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
            "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
            "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-ME0kITA1S"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Read in only the first 250 rows\n",
        "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
        "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
        "# Build the dataloader\n",
        "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adk0R3fgTA1S"
      },
      "source": [
        "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:         \n",
        "            batch_X, batch_y = batch\n",
        "            \n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            model.zero_grad()\n",
        "            \n",
        "            out=model.forward(batch_X)\n",
        "            \n",
        "            loss=loss_fn(out,batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # TODO: Complete this train method to train the model provided.\n",
        "            \n",
        "            total_loss += loss.data.item()\n",
        "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTUEfMIbTA1T",
        "outputId": "549424b7-1c2d-41a8-dc9a-f071be2b90fd"
      },
      "source": [
        "import torch.optim as optim\n",
        "from train.model import LSTMClassifier\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMClassifier(32, 100, 5000).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, BCELoss: 0.6946825623512268\n",
            "Epoch: 2, BCELoss: 0.6842933416366577\n",
            "Epoch: 3, BCELoss: 0.675960099697113\n",
            "Epoch: 4, BCELoss: 0.667292320728302\n",
            "Epoch: 5, BCELoss: 0.6573679685592652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYEbd8U-TA1V",
        "outputId": "b5e2a6c4-f4f1-474c-fe95-e579b90d30e0"
      },
      "source": [
        "# TODO: Deploy the trained model\n",
        "predictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Creating model with name: sagemaker-pytorch-2018-12-25-11-38-48-314\n",
            "INFO:sagemaker:Creating endpoint with name sagemaker-pytorch-2018-12-25-11-38-48-314\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaeEEd7FTA1W"
      },
      "source": [
        "import pickle\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")\n",
        "cache_file=\"preprocessed_data.pkl\"\n",
        "with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "train_X, test_X, train_y, test_y = cache_data['words_train'],cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDjJ3ajqTA1W"
      },
      "source": [
        "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
        "\n",
        "with open(os.path.join(data_dir, 'word_dict.pkl'), \"rb\") as f1:\n",
        "    word_dict=pickle.load(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjlnY5wqTA1W"
      },
      "source": [
        "import pandas as pd\n",
        "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4m4NRX1TA1W"
      },
      "source": [
        "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
        "\n",
        "def predict(data, rows=512):\n",
        "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
        "    predictions = np.array([])\n",
        "    for array in split_array:\n",
        "        predictions = np.append(predictions, predictor.predict(array))\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m00unoyITA1X"
      },
      "source": [
        "predictions = predict(test_X.values)\n",
        "predictions = [round(num) for num in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6GpAGhlTA1X",
        "outputId": "ef4e247e-fe76-4c4b-a5d1-9b2e6a14b4bb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test_y, predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84932"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFU9J3SbTA1Y"
      },
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_4uLv10TA1Y"
      },
      "source": [
        "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihB30MbNTA1Z"
      },
      "source": [
        "# TODO: Convert test_review into a form usable by the model and save the results in test_data\n",
        "test_data_review_to_words = review_to_words(test_review)\n",
        "\n",
        "test_data = [np.array(convert_and_pad(word_dict, test_data_review_to_words)[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBD_Cm60TA1Z",
        "outputId": "8f206d68-ad67-464b-de9b-4a8be866cd01"
      },
      "source": [
        "test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([   1, 1376,   49,   53,    3,    4,  878,  173,  392,  682,   29,\n",
              "         724,    2, 4420,  275, 2082, 1061,  760,    1,  582,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9yJJyqdTA1a",
        "outputId": "d0057d02-e517-417d-8434-dc26f111c031"
      },
      "source": [
        "predictor.predict(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(0.5848401, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcQFww2GTA1b"
      },
      "source": [
        "Since the return value of our model is close to `1`, we can be certain that the review we submitted is positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttQ8W9cMTA1b",
        "outputId": "b4f96768-dffe-48de-a0b0-0d12399b322e"
      },
      "source": [
        "estimator.delete_endpoint()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Deleting endpoint with name: sagemaker-pytorch-2018-12-25-11-38-48-314\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m12jCnblTA1c",
        "outputId": "ed03920f-5b6e-4dcc-cd38-9ba8707e0a2e"
      },
      "source": [
        "!pygmentize serve/predict.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\r\n",
            "\r\n",
            "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\r\n",
            "\r\n",
            "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
            "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "\r\n",
            "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
            "    model_info = {}\r\n",
            "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
            "        model_info = torch.load(f)\r\n",
            "\r\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
            "\r\n",
            "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
            "\r\n",
            "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\r\n",
            "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
            "        model.load_state_dict(torch.load(f))\r\n",
            "\r\n",
            "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n",
            "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
            "        model.word_dict = pickle.load(f)\r\n",
            "\r\n",
            "    model.to(device).eval()\r\n",
            "\r\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
            "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "        \u001b[34mreturn\u001b[39;49;00m data\r\n",
            "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "\r\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "    \r\n",
            "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
            "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \r\n",
            "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\r\n",
            "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\r\n",
            "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\r\n",
            "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\r\n",
            "\r\n",
            "    data_X = \u001b[36mNone\u001b[39;49;00m\r\n",
            "    data_len = \u001b[36mNone\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\r\n",
            "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\r\n",
            "    data_pack = np.hstack((data_len, data_X))\r\n",
            "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\r\n",
            "    \r\n",
            "    data = torch.from_numpy(data_pack)\r\n",
            "    data = data.to(device)\r\n",
            "\r\n",
            "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\r\n",
            "    model.eval()\r\n",
            "\r\n",
            "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\r\n",
            "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\r\n",
            "\r\n",
            "    result = \u001b[36mNone\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mreturn\u001b[39;49;00m result\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STmwu8gzTA1d"
      },
      "source": [
        "import glob\n",
        "\n",
        "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
        "    \n",
        "    results = []\n",
        "    ground = []\n",
        "    \n",
        "    # We make sure to test both positive and negative reviews    \n",
        "    for sentiment in ['pos', 'neg']:\n",
        "        \n",
        "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
        "        files = glob.glob(path)\n",
        "        \n",
        "        files_read = 0\n",
        "        \n",
        "        print('Starting ', sentiment, ' files')\n",
        "        \n",
        "        # Iterate through the files and send them to the predictor\n",
        "        for f in files:\n",
        "            with open(f) as review:\n",
        "                # First, we store the ground truth (was the review positive or negative)\n",
        "                if sentiment == 'pos':\n",
        "                    ground.append(1)\n",
        "                else:\n",
        "                    ground.append(0)\n",
        "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
        "                review_input = review.read().encode('utf-8')\n",
        "                # Send the review to the predictor and store the results\n",
        "                results.append(float(predictor.predict(review_input)))\n",
        "                \n",
        "            # Sending reviews to our endpoint one at a time takes a while so we\n",
        "            # only send a small number of reviews\n",
        "            files_read += 1\n",
        "            if files_read == stop:\n",
        "                break\n",
        "            \n",
        "    return ground, results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6OfMlRlTA1e",
        "outputId": "1f375440-ee44-4037-a4bd-2d17172c2004"
      },
      "source": [
        "ground, results = test_reviews()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting  pos  files\n",
            "Starting  neg  files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyzbwWtrTA1e",
        "outputId": "ad17fb14-2bbe-4398-cdcb-c9d3ceba4e48"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(ground, results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqvPBbasTA1e"
      },
      "source": [
        "As an additional test, we can try sending the `test_review` that we looked at earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17zu8y4mTA1e",
        "outputId": "62606b66-89bf-4bb6-b3c7-11bb437e9de1"
      },
      "source": [
        "predictor.predict(test_review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYFFu8KhTA1f",
        "outputId": "6dc21642-3342-4392-e8ad-f5d47438d6f0"
      },
      "source": [
        "predictor.endpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sagemaker-pytorch-2018-12-25-12-16-23-374'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7rTr2xSTA1l",
        "outputId": "b2ba72da-513f-48d9-dfcd-9977856f2f07"
      },
      "source": [
        "predictor.delete_endpoint()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Deleting endpoint with name: sagemaker-pytorch-2018-12-25-12-16-23-374\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGlp7DayTA1m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}